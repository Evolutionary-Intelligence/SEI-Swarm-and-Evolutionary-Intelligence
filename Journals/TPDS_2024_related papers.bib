
@article{sun_adaptive_2024,
	title = {Adaptive {Auto}-{Tuning} {Framework} for {Global} {Exploration} of {Stencil} {Optimization} on {GPUs}},
	volume = {35},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/10287597},
	doi = {10.1109/TPDS.2023.3325630},
	abstract = {Stencil computations are widely used in high performance computing (HPC) applications. Many HPC platforms utilize the high computation capability of GPUs to accelerate stencil computations. In recent years, stencils have become more diverse in terms of stencil order, memory accesses and computation patterns. To adapt diverse stencils to GPUs, a variety of optimization techniques have been proposed. Due to the diversity of stencil patterns and GPU architectures, no single optimization technique fits all stencils. Therefore, stencil auto-tuning mechanisms have been proposed to conduct parameter search for a given combination of optimization techniques. However, parameter search for an inappropriate optimization combination (OC) misses the globally optimal solution. To address the above problems, we propose GSTuner, an adaptive auto-tuning framework that efficiently determines the optimal parameter setting of the global optimization space for stencils on GPUs. Specifically, GSTuner represents stencil patterns as neighboring features and unifies feature vectors of OCs through data pre-processing. In addition, GSTuner samples parameter settings from superior OCs via the quota-based reward policy and regression mechanisms. After that, GSTuner employs the genetic algorithm that considers sub-population similarity to reduce the cost of evolutionary search. The experiment results show that GSTuner can identify better performing settings with higher auto-tuning speed compared to the state-of-the-art works.},
	number = {1},
	urldate = {2026-01-28},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Sun, Qingxiao and Liu, Yi and Yang, Hailong and Jiang, Zhonghui and Luan, Zhongzhi and Qian, Depei},
	month = jan,
	year = {2024},
	keywords = {auto-tuning, deep learning, genetic algorithm, GPU, Graphics processing units, Hardware, Kernel, Merging, Optimization, Parallel processing, performance prediction, Registers, Stencil computation},
	pages = {20--33},
	file = {Full Text PDF:D\:\\44444 Research\\zotero data\\storage\\U8WWEB46\\Sun 等 - 2024 - Adaptive Auto-Tuning Framework for Global Exploration of Stencil Optimization on GPUs.pdf:application/pdf},
}

@article{zhang_3d_2024,
	title = {A {3D} {Hybrid} {Optical}-{Electrical} {NoC} {Using} {Novel} {Mapping} {Strategy} {Based} {DCNN} {Dataflow} {Acceleration}},
	volume = {35},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/10518057},
	doi = {10.1109/TPDS.2024.3394747},
	abstract = {A large number of multiply-accumulate operations and memory accesses required in deep convolutional neural networks (DCNN) leads to high latency and energy consumption (EC), that hinder their further applications. Dataflow-based acceleration schemes reduce memory accesses by leveraging reusable data in DCNNs. Row Stationary (RS) dataflow is a more advanced dataflow. In the convolutional layer acceleration of RS dataflow, the flexibility of mapping from logical processing element (LPE) sets to physical PE sets is relatively poor. The utilization of processing elements (PEs) is low. In this article, a novel mapping strategy based on genetic algorithm (GAMS) with the goal of optimizing EC is proposed. GAMS is designed to address the energy inefficiencies faced when mapping RS dataflow. A 3D hybrid optical-electrical Network-on-Chip (3DHOENoC) is proposed to further improve the communication efficiency, energy efficiency and the processing speed of DCNN. Simulation and evaluation results show that GAMS can achieve better mapping flexibility, higher PEs utilization and 15.9\% improvement of execution speed on average. In addition, the execution time (ET) performance of processing the DCNN can be further improved by adopting the 3DHOENoC architecture with better communication parallelism.},
	number = {7},
	urldate = {2026-01-28},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhang, Bowen and Gu, Huaxi and Zhang, Grace Li and Yang, Yintang and Ma, Ziteng and Schlichtmann, Ulf},
	month = jul,
	year = {2024},
	keywords = {3DNoC, AI, Arrays, Convolution, Convolutional neural networks, dataflow, DCNN acceleration, mapping strategy, ONoC, Optical filters, Optical network units, Optical resonators, Three-dimensional displays},
	pages = {1139--1154},
}

@article{duan_distributed_2024,
	title = {Distributed {Evolution} {Strategies} {With} {Multi}-{Level} {Learning} for {Large}-{Scale} {Black}-{Box} {Optimization}},
	volume = {35},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/10621616},
	doi = {10.1109/TPDS.2024.3437688},
	abstract = {In the post-Moore era, main performance gains of black-box optimizers are increasingly depending on parallelism, especially for large-scale optimization (LSO). Here we propose to parallelize the well-established covariance matrix adaptation evolution strategy (CMA-ES) and in particular its one latest LSO variant called limited-memory CMA-ES (LM-CMA). To achieve efficiency while approximating its powerful invariance property, we present a multilevel learning-based meta-framework for distributed LM-CMA. Owing to its hierarchically organized structure, Meta-ES is well-suited to implement our distributed meta-framework, wherein the outer-ES controls strategy parameters while all parallel inner-ESs run the serial LM-CMA with different settings. For the distribution mean update of the outer-ES, both the elitist and multi-recombination strategy are used in parallel to avoid stagnation and regression, respectively. To exploit spatiotemporal information, the global step-size adaptation combines Meta-ES with the parallel cumulative step-size adaptation. After each isolation time, our meta-framework employs both the structure and parameter learning strategy to combine aligned evolution paths for CMA reconstruction. Experiments on a set of large-scale benchmarking functions with memory-intensive evaluations, arguably reflecting many data-driven optimization problems, validate the benefits (e.g., effectiveness w.r.t. solution quality, and adaptability w.r.t. second-order learning) and costs of our meta-framework.},
	number = {11},
	urldate = {2026-01-28},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Duan, Qiqi and Shao, Chang and Zhou, Guochen and Zhang, Minghan and Zhao, Qi and Shi, Yuhui},
	month = nov,
	year = {2024},
	keywords = {Black-box optimization (BBO), Closed box, Complexity theory, Computational modeling, Distributed computing, distributed optimization, evolution strategies (ESs), large-scale optimization (LSO), Optimization, Parallel processing, parallelism, Runtime},
	pages = {2087--2101},
	file = {Full Text PDF:D\:\\44444 Research\\zotero data\\storage\\94GNI6IH\\Duan 等 - 2024 - Distributed Evolution Strategies With Multi-Level Learning for Large-Scale Black-Box Optimization.pdf:application/pdf},
}
